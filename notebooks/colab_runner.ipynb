{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clustering Pipelines - Colab Runner\n",
        "Run the repository's clustering pipelines inside Google Colab or any standard Jupyter runtime. Execute the cells in order, customizing the configuration blocks as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Clone the repository when starting from a blank Colab runtime\n",
        "If you opened this notebook directly from GitHub, the Python modules and datasets are not present yet. Fill in `REPO_URL` with your clone URL and run the next cell to fetch the repo into `/content/clustering`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"\"  # e.g. \"https://github.com/<user>/clustering.git\"\n",
        "TARGET_DIR = Path(\"/content/clustering\")\n",
        "\n",
        "if REPO_URL:\n",
        "    if TARGET_DIR.exists():\n",
        "        print(f\"Removing existing directory: {TARGET_DIR}\")\n",
        "        shutil.rmtree(TARGET_DIR)\n",
        "    print(f\"Cloning {REPO_URL} -> {TARGET_DIR}\")\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL, str(TARGET_DIR)], check=True)\n",
        "    os.chdir(TARGET_DIR)\n",
        "    print(f\"Changed working directory to {TARGET_DIR}\")\n",
        "else:\n",
        "    print(\"Skipping clone. Set REPO_URL above when running in a fresh Colab session.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Python dependencies\n",
        "This project only needs pandas, scikit-learn, and matplotlib; keep this cell even if your runtime already has them so versions match the repo's expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --quiet pandas scikit-learn matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure dataset and pipelines to run\n",
        "Set `dataset_metadata_path` to any of the JSON files under `datasets/`. Update `pipelines_to_run` with one or more of `\"knn\"`, `\"kmeans\"`, or `\"dbscan\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "WORK_DIR = Path.cwd()\n",
        "SRC_DIR = WORK_DIR / \"src\"\n",
        "if not SRC_DIR.exists():\n",
        "    raise FileNotFoundError(\"Could not find the 'src' directory. Ensure the notebook runs from the repository root or clone it in the previous cell.\")\n",
        "\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.append(str(SRC_DIR))\n",
        "\n",
        "from common.dataset_metadata import load_dataset_metadata\n",
        "\n",
        "dataset_metadata_path = WORK_DIR / \"datasets\" / \"raw\" / \"bank-full.json\"\n",
        "pipelines_to_run = [\"knn\", \"kmeans\", \"dbscan\"]\n",
        "\n",
        "dataset_meta = load_dataset_metadata(dataset_metadata_path, base_dir=WORK_DIR)\n",
        "print(f\"Dataset: {dataset_meta.name}\")\n",
        "print(f\"Rows: using file at {dataset_meta.dataset_path}\")\n",
        "print(f\"Selected pipelines: {pipelines_to_run}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the selected pipelines\n",
        "This reuses the project's existing pipeline runners (identical to invoking `python src/main.py`). Results and trained models are written under `results/<pipeline>/<config>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from main import run_knn, run_kmeans, run_dbscan\n",
        "\n",
        "runners = {\n",
        "    \"knn\": run_knn,\n",
        "    \"kmeans\": run_kmeans,\n",
        "    \"dbscan\": run_dbscan,\n",
        "}\n",
        "\n",
        "for name in pipelines_to_run:\n",
        "    if name not in runners:\n",
        "        raise ValueError(f\"Unknown pipeline requested: {name}\")\n",
        "\n",
        "print(\"\\n=== Starting pipeline runs ===\")\n",
        "for name in pipelines_to_run:\n",
        "    print(f\"\\n>>> Running {name.upper()} pipeline\")\n",
        "    runners[name](WORK_DIR, dataset_meta)\n",
        "print(\"\\nAll requested pipelines completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect generated artifacts\n",
        "Each pipeline stores `metadata.json` and `model.pkl` under the results directory. Use the helper below to list what was produced.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "results_root = WORK_DIR / \"results\"\n",
        "if not results_root.exists():\n",
        "    raise FileNotFoundError(\"No results directory found yet. Run at least one pipeline first.\")\n",
        "\n",
        "for metadata_path in sorted(results_root.glob(\"*/**/metadata.json\")):\n",
        "    rel = metadata_path.relative_to(WORK_DIR)\n",
        "    print(f\"âœ” {rel}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
